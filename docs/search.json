[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sai Ram Kuchana",
    "section": "",
    "text": "Linkedin\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n  \n  \nHi, I’m Ram!\nHere, I post my projects whenever i have done something new and got time."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Sai Ram Kuchana",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\nFine tuning a Representation Model for Binary Sentiment Classification\n\n\n\nTransformers\n\n\ndatasets\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFew-Shot Text Classification using SetFit Framework\n\n\n\nsetfit\n\n\ndatasets\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a pre-trained BERT model for text prediction and analysing its self-attention scores\n\n\n\nPyTorch\n\n\nPIL\n\n\nTransformers\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping neural network models for classification problems using PyTorch\n\n\n\nPyTorch\n\n\nNumPy\n\n\nMatplotlib\n\n\nscikit-learn\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding neural network models to identify the type of clothing in images\n\n\n\nPyTorch\n\n\nMatplotlib\n\n\nPandas\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-learning model for recognizing handwritten digit\n\n\n\nNumPy\n\n\nMatplotlib\n\n\nscikit-learn\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an AI to play a game using reinforcement learning\n\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating parse trees and extracting noun phrases\n\n\n\nNLTK\n\n\nPython\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SQL to find the missing packages in the mail delivery service’s database\n\n\n\nSQL\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoft deletions, views, and triggers in the context of a museum’s database\n\n\n\nSQL\n\n\nAll\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a dashboard for the sales using Tableau\n\n\n\nTableau\n\n\nAll\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/SQLPackages/packages.html",
    "href": "projects/SQLPackages/packages.html",
    "title": "Using SQL to find the missing packages in the mail delivery service’s database",
    "section": "",
    "text": "Imagine you are a mail clerk for the city of Boston who oversees the delivery of mail across the city. For the most part, all packages sent are eventually delivered. However, every once in while, a mystery lands on your desk: a missing package! For each customer that comes to you with a report of a missing package, your task is to help each customer find their missing package and answer their relevant questions using just the information in the mail delivery service’s database, packages.db, which contains data on the transit of packages around the city.\nThe specific problems to solve and the schema of the database are described below.\n\n\nThe first report of a missing package comes from Anneke. Anneke walks up to your counter and tells you the following:\n\n\n\n\n\n\nClerk, my name’s Anneke. I live over at 900 Somerville Avenue. Not long ago, I sent out a special letter. It’s meant for my friend Varsha. She’s starting a new chapter of her life at 2 Finnegan Street, uptown. (That address, let me tell you: it was a bit tricky to get right the first time.) The letter is a congratulatory note—a cheery little paper hug from me to her, to celebrate this big move of hers. Can you check if it’s made its way to her yet?\n\n\n\nYour job is to find out:\n\nAt what type of address did the Lost Letter end up?\nAt what address did the Lost Letter end up?\n\n\n\n\nThe second report of a missing package comes from a mysterious fellow from out of town. They walk up to your counter and tell you the following:\n\n\n\n\n\n\nGood day to you, deliverer of the mail. You might remember that not too long ago I made my way over from the town of Fiftyville. I gave a certain box into your reliable hands and asked you to keep things low. My associate has been expecting the package for a while now. And yet, it appears to have grown wings and flown away. Ha! Any chance you could help clarify this mystery? Afraid there’s no “From” address. It’s the kind of parcel that would add a bit more… quack to someone’s bath times, if you catch my drift.\n\n\n\nYour job is to find out:\n\nAt what type of address did the Devious Delivery end up?\nWhat were the contents of the Devious Delivery?\n\n\n\n\nThe third report of a missing package comes from a grandparent who lives down the street from the post office. They approach your counter and tell you the following:\n\n\n\n\n\n\nOh, excuse me, Clerk. I had sent a mystery gift, you see, to my wonderful granddaughter, off at 728 Maple Place. That was about two weeks ago. Now the delivery date has passed by seven whole days and I hear she still waits, her hands empty and heart filled with anticipation. I’m a bit worried wondering where my package has gone. I cannot for the life of me remember what’s inside, but I do know it’s filled to the brim with my love for her. Can we possibly track it down so it can fill her day with joy? I did send it from my home at 109 Tileston Street.\n\n\n\nYour job is to find out:\n\nWhat are the contents of the Forgotten Gift?\nWho has the Forgotten Gift?"
  },
  {
    "objectID": "projects/SQLPackages/packages.html#problem-1-the-missing-letter",
    "href": "projects/SQLPackages/packages.html#problem-1-the-missing-letter",
    "title": "Using SQL to find the missing packages in the mail delivery service’s database",
    "section": "",
    "text": "The first report of a missing package comes from Anneke. Anneke walks up to your counter and tells you the following:\n\n\n\n\n\n\nClerk, my name’s Anneke. I live over at 900 Somerville Avenue. Not long ago, I sent out a special letter. It’s meant for my friend Varsha. She’s starting a new chapter of her life at 2 Finnegan Street, uptown. (That address, let me tell you: it was a bit tricky to get right the first time.) The letter is a congratulatory note—a cheery little paper hug from me to her, to celebrate this big move of hers. Can you check if it’s made its way to her yet?\n\n\n\nYour job is to find out:\n\nAt what type of address did the Lost Letter end up?\nAt what address did the Lost Letter end up?"
  },
  {
    "objectID": "projects/SQLPackages/packages.html#problem-2-the-devious-delivery",
    "href": "projects/SQLPackages/packages.html#problem-2-the-devious-delivery",
    "title": "Using SQL to find the missing packages in the mail delivery service’s database",
    "section": "",
    "text": "The second report of a missing package comes from a mysterious fellow from out of town. They walk up to your counter and tell you the following:\n\n\n\n\n\n\nGood day to you, deliverer of the mail. You might remember that not too long ago I made my way over from the town of Fiftyville. I gave a certain box into your reliable hands and asked you to keep things low. My associate has been expecting the package for a while now. And yet, it appears to have grown wings and flown away. Ha! Any chance you could help clarify this mystery? Afraid there’s no “From” address. It’s the kind of parcel that would add a bit more… quack to someone’s bath times, if you catch my drift.\n\n\n\nYour job is to find out:\n\nAt what type of address did the Devious Delivery end up?\nWhat were the contents of the Devious Delivery?"
  },
  {
    "objectID": "projects/SQLPackages/packages.html#problem-3-the-forgotten-gift",
    "href": "projects/SQLPackages/packages.html#problem-3-the-forgotten-gift",
    "title": "Using SQL to find the missing packages in the mail delivery service’s database",
    "section": "",
    "text": "The third report of a missing package comes from a grandparent who lives down the street from the post office. They approach your counter and tell you the following:\n\n\n\n\n\n\nOh, excuse me, Clerk. I had sent a mystery gift, you see, to my wonderful granddaughter, off at 728 Maple Place. That was about two weeks ago. Now the delivery date has passed by seven whole days and I hear she still waits, her hands empty and heart filled with anticipation. I’m a bit worried wondering where my package has gone. I cannot for the life of me remember what’s inside, but I do know it’s filled to the brim with my love for her. Can we possibly track it down so it can fill her day with joy? I did send it from my home at 109 Tileston Street.\n\n\n\nYour job is to find out:\n\nWhat are the contents of the Forgotten Gift?\nWho has the Forgotten Gift?"
  },
  {
    "objectID": "projects/setfitFewshot/setfitFewshot.html",
    "href": "projects/setfitFewshot/setfitFewshot.html",
    "title": "Few-Shot Text Classification using SetFit Framework",
    "section": "",
    "text": "This project uses the SetFit (Sentence Transformer Fine-Tuning) framework to perform few-shot text classification. Few-shot learning allows a model to learn well with very little labeled data, which is especially useful when collecting large datasets is difficult. In this project, we use the IMDb movie reviews dataset to classify reviews as positive or negative, aiming to train an effective model using only a small number of labeled examples for each class.\nSetFit combines the strengths of pre-trained sentence transformers with contrastive learning to fine-tune sentence embeddings for specific tasks. This makes it possible to learn efficiently from just a few examples. The framework fine-tunes the embeddings by using pairs of similar and different sentences, then trains a simple classifier on top of the embeddings for the classification task.\nWe go through the following primary steps:"
  },
  {
    "objectID": "projects/setfitFewshot/setfitFewshot.html#imdb-dataset",
    "href": "projects/setfitFewshot/setfitFewshot.html#imdb-dataset",
    "title": "Few-Shot Text Classification using SetFit Framework",
    "section": "1.1 IMDb dataset",
    "text": "1.1 IMDb dataset\nThe IMDb dataset consists of movie reviews labeled as either positive or negative. The original dataset has balanced train and test datasets, each containing 25,000 labeled samples (reviews) and an additional 50,000 unlabeled samples for unsupervised learning.\nlet’s load and explore the original IMDb dataset.\n\n\nCode\nfrom datasets import load_dataset, concatenate_datasets\n\n# Load the IMDb dataset\nimdb_dataset = load_dataset(\"imdb\")\n\n\n\n\nCode\n# Print the imdb dataset\n\nprint(imdb_dataset)\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n\n\nCode\n# Inspecting the dataset structure\n\nprint(imdb_dataset[\"train\"].features)\n\n\n{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n\n\nCode\n# Checking the first review to see what it looks like\n\nprint(imdb_dataset[\"train\"][0][\"text\"])\n\n\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n\n\nCode\n# Checking the label of the first review\n\nprint(imdb_dataset[\"train\"][0][\"label\"])\n\n\n0"
  },
  {
    "objectID": "projects/setfitFewshot/setfitFewshot.html#few-shot-setting",
    "href": "projects/setfitFewshot/setfitFewshot.html#few-shot-setting",
    "title": "Few-Shot Text Classification using SetFit Framework",
    "section": "1.2 Few-shot setting",
    "text": "1.2 Few-shot setting\nHowever, to simulate the few-shot setting, we will sample only 32 examples (16 positive, 16 negative) from the original dataset to demonstrate how the SetFit framework can effectively perform few-shot learning using contrastive learning.\nWe also create a balanced test dataset with 2,000 examples from the original dataset to reduce the computational resources needed for evaluating the model.\n\n\nCode\n# Define a function to balance the dataset\ndef balance_dataset(dataset, label_col, num_samples):\n    # Filter positive and negative examples\n    positive_samples = dataset.filter(lambda example: example[label_col] == 1)\n    negative_samples = dataset.filter(lambda example: example[label_col] == 0)\n\n    # Subsample both to the desired number\n    positive_samples = positive_samples.shuffle(seed=42).select(range(num_samples // 2))\n    negative_samples = negative_samples.shuffle(seed=42).select(range(num_samples // 2))\n\n    # Concatenate positive and negative examples to form a balanced dataset\n    balanced_dataset = concatenate_datasets([positive_samples, negative_samples]).shuffle(seed=42)\n\n    return balanced_dataset\n\n\n\n\nCode\n# Create a balanced train and test dataset\ntrain_data = balance_dataset(imdb_dataset[\"train\"], \"label\", 32)\ntest_data = balance_dataset(imdb_dataset[\"test\"], \"label\", 2000)\n\n# Checking the datasets\nprint(f\"train_data:\\n {train_data}\", end='\\n\\n')\nprint(f\"test_data:\\n {test_data}\")\n\n\ntrain_data:\n Dataset({\n    features: ['text', 'label'],\n    num_rows: 32\n})\n\ntest_data:\n Dataset({\n    features: ['text', 'label'],\n    num_rows: 2000\n})"
  },
  {
    "objectID": "projects/setfitFewshot/setfitFewshot.html#training-the-model",
    "href": "projects/setfitFewshot/setfitFewshot.html#training-the-model",
    "title": "Few-Shot Text Classification using SetFit Framework",
    "section": "2.1 Training the model",
    "text": "2.1 Training the model\n\n\nCode\nfrom setfit import TrainingArguments as SetFitTrainingArguments\nfrom setfit import Trainer as SetFitTrainer\n\n# Define training arguments\nargs = SetFitTrainingArguments(\n    num_epochs=3, # The number of epochs to use for contrastive learning\n    num_iterations=20  # The number of text pairs to generate\n)\n\n\nThe SetFitTrainingArguments class is used to specify how the model should be trained. It’s similar to training configurations in other frameworks like transformers.\nnum_epochs=3: This specifies that the training process will run for 3 epochs. One epoch means that the entire training dataset will pass through the model once.\nnum_iterations=20: This refers to the number of contrastive learning steps or the number of text pairs the model should learn from.\nContrastive Learning is a self-supervised learning technique where the model learns to differentiate between similar and dissimilar pairs of inputs. It’s widely used to improve the quality of sentence embeddings.\n\n\nCode\n# Create trainer\ntrainer = SetFitTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    metric=\"f1\"\n)\n\n\nThe SetFitTrainer is the main class responsible for orchestrating the training process. It takes care of the training loop, optimization, and evaluation.\n\n\nCode\n# Training loop\ntrainer.train()\n\n\n***** Running training *****\n  Num unique pairs = 1280\n  Batch size = 16\n  Num epochs = 3\n\ntrainer.train() triggers the training loop where the model will learn from the sampled training data (16 examples per class) for 3 epochs, applying contrastive learning over 20 iterations per batch.\nNotice that output mentions that 1,280 sentence pairs were generated from the 32 samples (16 positive and 16 negative)we provided for fine-tuning the SentenceTransformer model."
  },
  {
    "objectID": "projects/setfitFewshot/setfitFewshot.html#evaluating-the-model",
    "href": "projects/setfitFewshot/setfitFewshot.html#evaluating-the-model",
    "title": "Few-Shot Text Classification using SetFit Framework",
    "section": "2.2 Evaluating the model",
    "text": "2.2 Evaluating the model\n\n\nCode\n# Evaluate the model on our test data\ntrainer.evaluate()\n\n\n***** Running evaluation *****\n\n{'f1': 0.8772504091653028}\nAfter training, the model is evaluated on the test data (test_data) using the F1 score. The goal is to assess how well the model performs in classifying movie reviews as positive or negative based on the few-shot training it received.\nThis highlights the power of few-shot learning with SetFit. With only a limited amount of labeled data (32 labeled samples), we achieved an impressive F1 score of 0.87"
  },
  {
    "objectID": "projects/ParseTrees/ParsetreesNnounphrases.html",
    "href": "projects/ParseTrees/ParsetreesNnounphrases.html",
    "title": "Generating parse trees and extracting noun phrases",
    "section": "",
    "text": "In Natural Language Processing (NLP), parsing refers to the process of analyzing a sentence to identify its grammatical structure. This is useful for a number of reasons, such as understanding sentence structure, resolving ambiguities in sentences, and better extraction of information from sentences.\nIn this project, we will work with the following English sentences and apply the context-free grammar (CFG) formalism to generate a parse tree or syntactic tree that represents the syntactic structure of the given sentences. Then, we will extract noun phrases from those sentences.\nSentences to analyze:"
  },
  {
    "objectID": "projects/ParseTrees/ParsetreesNnounphrases.html#cfg-rules",
    "href": "projects/ParseTrees/ParsetreesNnounphrases.html#cfg-rules",
    "title": "Generating parse trees and extracting noun phrases",
    "section": "1.1 CFG rules",
    "text": "1.1 CFG rules\nA rewriting rule in a CFG has the following form:\n𝐴 → 𝛼\nWhere: * 𝐴 is a non-terminal symbol and * 𝛼 is a string consisting of terminal and/or non-terminal symbols.\nNow, let’s define a concrete set of rules for generating terminal symbols and non-terminal symbols.\n\n1.1.1 Terminal symbols\nTerminal symbols represent the words in the sentence. The rules for generating these terminal symbols are defined in the global variable TERMINALS, which includes all the words of the given sentences to analyze. Notice that Adj is a nonterminal symbol that generates adjectives, Adv generates adverbs, Conj generates conjunctions, Det generates determiners, N generates nouns (spread across multiple lines for readability), P generates prepositions, and V generates verbs. The vertical bar | denotes all the terminal symbol alternatives for that particular non-terminal symbol.\n\n\nCode\nTERMINALS = \"\"\"\nAdj -&gt; \"country\" | \"dreadful\" | \"enigmatical\" | \"little\" | \"moist\" | \"red\"\nAdv -&gt; \"down\" | \"here\" | \"never\"\nConj -&gt; \"and\" | \"until\"\nDet -&gt; \"a\" | \"an\" | \"his\" | \"my\" | \"the\"\nN -&gt; \"armchair\" | \"companion\" | \"day\" | \"door\" | \"hand\" | \"he\" | \"himself\"\nN -&gt; \"holmes\" | \"home\" | \"i\" | \"mess\" | \"paint\" | \"palm\" | \"pipe\" | \"she\"\nN -&gt; \"smile\" | \"thursday\" | \"walk\" | \"we\" | \"word\" \nP -&gt; \"at\" | \"before\" | \"in\" | \"of\" | \"on\" | \"to\" \nV -&gt; \"arrived\" | \"came\" | \"chuckled\" | \"had\" | \"lit\" | \"said\" | \"sat\"\nV -&gt; \"smiled\" | \"tell\" | \"were\" \n\"\"\"\n\n\n\n\n1.1.2 Non-terminal symbols\nNon-terminal symbols are syntactic variables that represent sets of strings. Common non-terminals include parts of speech (e.g., noun, verb) and phrases (e.g., noun phrase, verb phrase). These symbols are defined in the global variable called NONTERMINALS and it contains all the rules for generating non-terminal symbols.\nThe description of symbols is as follows: S represents a complete sentence, NP represents a noun phrase, VP represents a verb phrase, PP represents a prepositional phrase, Det represents a determiner, and AP represents an adjective phrase. Each alternative separated by the vertical bar | represents a different way that a non-terminal can be replaced by a sequence of terminals and/or non-terminals. We can notice that all these rules are self-explanatory. For example, a rule like AP → Adj | Adj AP indicates that an adjective phrase can be formed either by an adjective or by an adjective followed by an adjective phrase.\n\n\nCode\nNONTERMINALS = \"\"\"\nS -&gt; NP VP | VP NP | S Conj S\n\nNP -&gt; N | Det N | NP PP | Det AP N\nVP -&gt; V | V NP | V PP | Adv VP | VP Adv\nAP -&gt; Adj | Adj AP\nPP -&gt; P NP\n\"\"\""
  },
  {
    "objectID": "projects/MNIST/mnist.html",
    "href": "projects/MNIST/mnist.html",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "",
    "text": "In this project, we will build a classifier that takes an image of a handwritten digit and recognizes the digit present in the image. Convolutional Neural Networks (CNNs) are usually preferred for such tasks because they are specifically designed to handle the spatial and hierarchical nature of image data, making them highly effective.\nHowever, in this project, just for the sake of exploring a new technique, let’s explore a simple strategy, a non-learning method which is based on finding the nearest neighbor(s) (NN) to build the image classifier and see how it performs.\nAs brute force technique is used to find the nearest neighbor in our baseline NN classifier, let’s first work with a smaller dataset derived from our original dataset to reduce the computational time and make the algorithm work initially. Then, we will look at methods to improve the classification time that reduces the nearest neighbor search time.\nFinally, note that this project is not about finding a good classifier for our dataset. Its rather about exploring some other technique and reasoning why it works and why it does not work.\nCode\n# load in the required modules\n\n%matplotlib inline\nimport numpy as np  \nimport matplotlib.pyplot as plt \nimport time\nimport gzip, os\nfrom sklearn.neighbors import KDTree"
  },
  {
    "objectID": "projects/MNIST/mnist.html#explore-the-original-dataset",
    "href": "projects/MNIST/mnist.html#explore-the-original-dataset",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "1.1 Explore the original dataset",
    "text": "1.1 Explore the original dataset\n\n\nCode\n# Print out the shape and no. of labels of the original dataset\n\nprint(\"Shape of the training dataset: \", np.shape(original_train_data))\nprint(\"Number of training labels: \", len(original_train_labels), end='\\n\\n')\n\nprint(\"Shape of the testing dataset: \", np.shape(original_test_data))\nprint(\"Number of testing labels: \", len(original_test_labels))\n\n\nShape of the training dataset:  (60000, 784)\nNumber of training labels:  60000\n\nShape of the testing dataset:  (10000, 784)\nNumber of testing labels:  10000\nWe can see that each data point, i.e., a handwritten digit image in the dataset, is composed of 784 pixels and is stored as a vector with 784 coordinates/dimensions, where each coordinate has a numeric value ranging from 0 to 255.\nLet’s look at these numeric values by examining one of the images, say, the first image, in the dataset.\n\n\nCode\n# print out the the first data point in training data \n\noriginal_train_data[0]\n\n\narray([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.], dtype=float32)\n\n\nCode\n# Compute the number of images of each digit in the training and test datasets\n\ntrain_digits, train_counts = np.unique(original_train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(original_test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)))  \n\n\nTraining set distribution:\n{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n\nTest set distribution:\n{0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\nWe can also see that our dataset is imbalanced. Now, let’s visualize some random images from the dataset.\n\n\nCode\n# Reshape the data to be in the form of 28x28 images\nX_train_images = original_train_data.reshape(-1, 28, 28)\n\n# Select 16 random images to display\nrandom_indices = np.random.choice(X_train_images.shape[0], 16, replace=False)\nrandom_images = X_train_images[random_indices]\nrandom_labels = original_train_labels[random_indices]  # Corresponding labels\n\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\n\nfor i, ax in enumerate(axes.flat):\n    # Plot each image on a subplot\n    ax.imshow(random_images[i], cmap='gray')\n    ax.set_title(f\"Label: {random_labels[i]}\")\n    ax.axis('off') \n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/MNIST/mnist.html#prepare-a-balanced-dataset-from-the-original-dataset",
    "href": "projects/MNIST/mnist.html#prepare-a-balanced-dataset-from-the-original-dataset",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "1.2 Prepare a balanced dataset from the original dataset",
    "text": "1.2 Prepare a balanced dataset from the original dataset\nNow that we have explored our original dataset, let’s prepare a balanced dataset from it that contains 8000 samples for the training set and 2000 samples for the test set. Balanced dataset means samples are uniformly distributed across all labels (0 to 9) in both the sets. We define a function balance_dataset to carry out this task.\n\n\nCode\ndef balance_dataset(original_data, original_labels, samples_per_label):\n\n    balanced_data = []\n    balanced_labels = []\n\n    # Iterate over each class label (here 0 to 9)\n    for class_label in range(len(np.unique(original_labels))):\n        \n        # Find indices where the label equals the current class label\n        indices = np.where(original_labels == class_label)[0]\n\n        # Sample 'samples_per_class' indices uniformly from the current label\n        sampled_indices = np.random.choice(indices, size=samples_per_label, replace=False)\n\n        # Append the sampled data and labels to the balanced dataset\n        balanced_data.append(original_data[sampled_indices])\n        balanced_labels.append(original_labels[sampled_indices])\n\n    # Concatenate the lists of arrays into single arrays\n    balanced_data = np.concatenate(balanced_data, axis=0)\n    balanced_labels = np.concatenate(balanced_labels, axis=0)\n\n    # Shuffle the balanced dataset\n    shuffle_indices = np.random.permutation(len(balanced_data))\n    balanced_data = balanced_data[shuffle_indices]\n    balanced_labels = balanced_labels[shuffle_indices]\n\n    \n    return balanced_data, balanced_labels\n\n\n\n\nCode\n# preparing the balanced training and testing dataset\n\ntrain_data, train_labels = balance_dataset(original_train_data, original_train_labels, 800)\n\ntest_data, test_labels = balance_dataset(original_test_data, original_test_labels, 200)\n\n\nNow, let’s also explore our balanced dataset.\n\n\nCode\n# Compute the number of images of each digit in the training and test datasets\n\ntrain_digits, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Training set distribution:\")\nprint(dict(zip(train_digits, train_counts)), end='\\n\\n')\n\ntest_digits, test_counts = np.unique(test_labels, return_counts=True)\nprint(\"Test set distribution:\")\nprint(dict(zip(test_digits, test_counts)), end='\\n\\n')  \n\n\n# Print out the shape and no. of labels of the balanced dataset\n\nprint(\"Shape of the training dataset: \", np.shape(train_data))\nprint(\"Number of training labels: \", len(train_labels), end='\\n\\n')\n\nprint(\"Shape of the testing dataset: \", np.shape(test_data))\nprint(\"Number of testing labels: \", len(test_labels))\n\n\nTraining set distribution:\n{0: 800, 1: 800, 2: 800, 3: 800, 4: 800, 5: 800, 6: 800, 7: 800, 8: 800, 9: 800}\n\nTest set distribution:\n{0: 200, 1: 200, 2: 200, 3: 200, 4: 200, 5: 200, 6: 200, 7: 200, 8: 200, 9: 200}\n\nShape of the training dataset:  (8000, 784)\nNumber of training labels:  8000\n\nShape of the testing dataset:  (2000, 784)\nNumber of testing labels:  2000\nSo in summary, we have prepared the following balanced dataset:\n\nA training set comprising 8000 images, 800 per label, and\nA test set consisting of 2000 images, 200 per label"
  },
  {
    "objectID": "projects/MNIST/mnist.html#squared-euclidean-distance",
    "href": "projects/MNIST/mnist.html#squared-euclidean-distance",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "2.1 Squared Euclidean distance",
    "text": "2.1 Squared Euclidean distance\nTo compute nearest neighbor in our dataset, we first need to be able to compute distances between data points (i.e., images in this case). For this, we will use the most common distance function Euclidean distance.\nIf x is the vector representing an image and y is the corresponding label, then:\n\nData space: \\(x \\in \\mathbb{R}^{784}\\), a 784-dimensional vector consisting of numeric values ranging from 0 to 255.\nLabel space: \\(y = \\{0.....9\\}\\), representing the label of the image.\n\nSince we have 784-dimensional vectors to work with, the Euclidean distance between two 784-dimensional vectors, say \\(x, z \\in \\mathbb{R}^{784}\\), is:\n\\[\\|x - z\\| = \\sqrt{\\sum_{i=1}^{784} (x_i - z_i)^2}\\]\nwhere \\(x_i\\) and \\(z_i\\) represent the \\(i^{th}\\) coordinates of x and z, respectively.\nFor easier computation, we often omit the square root and simply compute Squared Euclidean distance:\n\\[\\|x - z\\|^2 = \\sum_{i=1}^{784} (x_i - z_i)^2\\]\nThe following squared_dist function computes the squared Euclidean distance between two vectors.\n\n\nCode\n# Computes squared Euclidean distance between two vectors.\n\ndef squared_dist(x,z):\n    return np.sum(np.square(x-z))\n\n\n\n2.1.1 Test the Euclidean distance function\nLet’s compute Euclidean distances of some random digits to see the squared_dist function in action before we use it in the NN classifier.\n\n\nCode\nprint('Examples:\\n')\n\n# Computing distances between random digits in our training set.\n\nprint(f\"Distance from digit {train_labels[3]} to digit {train_labels[5]} in our training set: {squared_dist(train_data[4],train_data[5])}\")\n\nprint(f\"Distance from digit {train_labels[3]} to digit {train_labels[1]} in our training set: {squared_dist(train_data[4],train_data[1])}\")\n\nprint(f\"Distance from digit {train_labels[3]} to digit {train_labels[7]} in our training set: {squared_dist(train_data[4],train_data[7])}\")\n\n\nExamples:\n\nDistance from digit 8 to digit 9 in our training set: 5624921.0\nDistance from digit 8 to digit 4 in our training set: 5024177.0\nDistance from digit 8 to digit 1 in our training set: 3521175.0"
  },
  {
    "objectID": "projects/MNIST/mnist.html#classification-time-of-baseline-nn",
    "href": "projects/MNIST/mnist.html#classification-time-of-baseline-nn",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "4.1 Classification time of baseline NN",
    "text": "4.1 Classification time of baseline NN\nSince the nearest neighbor classifier goes through the entire training set of 8000 images, searching for the nearest neighbor image for every single test image in the dataset of 2000 images, we should not expect testing to be very fast.\n\n\nCode\n# Compute the classification time of NN classifier\n\nprint(f\"Classification time of NN classifier: {round(t_after - t_before, 2)} sec\")\n\n\nClassification time of NN classifier: 57.33 sec"
  },
  {
    "objectID": "projects/MNIST/mnist.html#error-rate-of-baseline-nn",
    "href": "projects/MNIST/mnist.html#error-rate-of-baseline-nn",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "4.2 Error rate of baseline NN",
    "text": "4.2 Error rate of baseline NN\n\n\nCode\n# Compute the error rate \n\nerr_positions = np.not_equal(test_predictions, test_labels)\nerror = float(np.sum(err_positions))/len(test_labels)\n\nprint(f\"Error rate of nearest neighbor classifier: {round(error * 100, 2)}%\")\n\n\nError rate of nearest neighbor classifier: 5.45%\nThe error rate of the NN classifier is 5.45%.\nThis means that out of the 2000 points, NN misclassifies around 109 of them, which is not too bad for such a simple method."
  },
  {
    "objectID": "projects/MNIST/mnist.html#k-d-tree-algorithm",
    "href": "projects/MNIST/mnist.html#k-d-tree-algorithm",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "5.1 k-d tree algorithm",
    "text": "5.1 k-d tree algorithm\nA k-d tree (k-dimensional tree) is a data structure used to organize points in a k-dimensional space. First, the k-d tree is built by recursively partitioning the data into two halves along one of the k dimensions. Next, during a nearest neighbor search, the algorithm uses the triangle inequality to determine whether it can prune certain branches of the tree (i.e., avoid searching certain subspaces). This allows the k-d tree to avoid unnecessary calculations, improving search efficiency thereby decreasing the classification time.\nHere, we will use scikit-learn library to implement our k-d tree.\n\n\nCode\n## Build nearest neighbor structure on training data\n\nt_before = time.time()\nkd_tree = KDTree(train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(f\"Time taken to build the data structure: {round(t_training, 2)} sec\")\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\nkd_tree_predictions = train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(f\"Time taken to classify test set: {round(t_testing, 2)} sec\", end = '\\n\\n')\n\n## total classification time\n\nprint(f\"Overall classification time of k-d tree algorithm: {round(t_training + t_testing, 2)} sec\")\n\n\nTime taken to build the data structure: 0.58 sec\nTime taken to classify test set: 10.27 sec\n\nOverall classification time of k-d tree algorithm: 10.85 sec\nSo, the k-d tree has drastically reduced our classification time. It took just 10.27 seconds to classify the entire test set while taking only around 0.5 second to build the tree data structure. Therefore, the overall classification time is approximately 11 seconds.\nRemember, the k-d tree algorithm only speeds up the nearest neighbor search; it does not affect the prediction process itself. The predictions of our baseline NN classifier will be the same as those of the k-d tree NN classifier. Let’s test this.\n\n\nCode\n## Verify that the both predictions of baseline NN and k-d tree NN are the same\n\nprint(\"Does k-d tree produce same predictions as NN classifier? \", np.array_equal(test_predictions, kd_tree_predictions))\n\n\nDoes k-d tree produce same predictions as NN classifier?  True"
  },
  {
    "objectID": "projects/MNIST/mnist.html#k-d-tree-on-the-full-mnist-dataset",
    "href": "projects/MNIST/mnist.html#k-d-tree-on-the-full-mnist-dataset",
    "title": "Non-learning model for recognizing handwritten digit",
    "section": "5.2 k-d tree on the full MNIST dataset",
    "text": "5.2 k-d tree on the full MNIST dataset\nNow let’s also look at how our k-d tree algorithm works on the original dataset i.e. dataset with 60,000 training images and 10,000 testing images.\n\n\nCode\nt_before = time.time()\nkd_tree = KDTree(original_train_data)\nt_after = time.time()\n\n## Compute training time\nt_training = t_after - t_before\nprint(f\"Time to build data structure: {round(t_training, 2)} sec\")\n\n## Get nearest neighbor predictions on testing data\nt_before = time.time()\ntest_neighbors = np.squeeze(kd_tree.query(original_test_data, k=1, return_distance=False))\nkd_tree_predictions = original_train_labels[test_neighbors]\nt_after = time.time()\n\n## Compute testing time\nt_testing = t_after - t_before\nprint(f\"Time to classify test set: {round(t_testing, 2)} sec\", end = '\\n\\n')\n\n## total classification time\n\nprint(f\"Overall classification time of k-d tree algorithm: {round(t_training+t_testing, 2)} sec\")\n\n\n---------------------------------------------------------------------------\n\nKeyboardInterrupt                         Traceback (most recent call last)\n\nKeyboardInterrupt: \nThe above code is taking forever so i have terminated it."
  },
  {
    "objectID": "projects/FashionMNIST/fashionMNIST.html",
    "href": "projects/FashionMNIST/fashionMNIST.html",
    "title": "Building neural network models to identify the type of clothing in images",
    "section": "",
    "text": "In this project, we will build neural network models to classify the type of clothing in images from the FashionMNIST dataset.\nWe will start by building a simple neural network model that will serve as a baseline for comparison with a more complex model, such as a Convolutional Neural Network (CNN) model.\nCode\n# Import necessary libraries\n\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nimport pandas as pd\nfrom torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix"
  },
  {
    "objectID": "projects/FashionMNIST/fashionMNIST.html#define-print_train_time-and-accuracy_fn",
    "href": "projects/FashionMNIST/fashionMNIST.html#define-print_train_time-and-accuracy_fn",
    "title": "Building neural network models to identify the type of clothing in images",
    "section": "2.1 Define print_train_time() and accuracy_fn",
    "text": "2.1 Define print_train_time() and accuracy_fn\nWe will define a utility function print_train_time() that calculates the time taken for training, given the start and end times of the training process.\nWe will also define an accuracy evaluation metric using accuracy_fn(), which measures the proportion of correctly classified samples out of the total number of samples.\n\n\nCode\n# Utility function to print training time\n\ndef print_train_time(start: float, end: float, device: torch.device = None) -&gt; float:\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n\n# Accuracy calculation function\n\ndef accuracy_fn(y_true, y_pred) -&gt; float:\n    correct = torch.eq(y_true, y_pred).sum().item()\n    return (correct / len(y_pred)) * 100"
  },
  {
    "objectID": "projects/Bert/bertmodel.html",
    "href": "projects/Bert/bertmodel.html",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "",
    "text": "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model designed for various NLP tasks, including masked language modeling. In masked language modeling, BERT predicts missing or masked words within a text sequence by leveraging the surrounding context. The transformer architecture it uses consists of multiple layers of self-attention mechanisms and feedforward neural networks, which enable it to understand and model complex language patterns. The base BERT model contains 12 transformer layers (also known as transformer blocks), each with 12 self-attention heads, totaling 144 self-attention heads across the model.\nIn this project, we will:"
  },
  {
    "objectID": "projects/Bert/bertmodel.html#import-the-required-modules",
    "href": "projects/Bert/bertmodel.html#import-the-required-modules",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "1.1 Import the required modules",
    "text": "1.1 Import the required modules\n\n\nCode\n# Import the required modules\n\nimport os\nimport sys\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont\nfrom transformers import BertTokenizer, BertForMaskedLM\n\n\nWe will utilize the transformers library from Hugging Face, which provides pre-trained models and tokenizers for a variety of NLP tasks. Specifically, BertTokenizer is used to convert text into tokens that the BERT model can process, while BertForMaskedLM is the version of BERT specialized for masked language modeling."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#setup-constants",
    "href": "projects/Bert/bertmodel.html#setup-constants",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "1.2 Setup constants",
    "text": "1.2 Setup constants\n\n\nCode\n# Pre-trained masked language model\nMODEL = \"bert-base-uncased\"\n\n# Number of predictions to generate\nK = 3\n\n# Constants for generating attention diagrams\nFONT_PATH = \"OpenSans-Regular.ttf\"\nGRID_SIZE = 40\nPIXELS_PER_WORD = 200\n\n# Load the font\ntry:\n    FONT = ImageFont.truetype(FONT_PATH, 28)\nexcept IOError:\n    print(f\"Font not found at {FONT_PATH}. Loading default font\")\n    FONT = ImageFont.load_default()\n\n\nWe are using the bert-base-uncased version of BERT, where all text is converted to lowercase before tokenization, meaning no distinction is made between uppercase and lowercase letters. The constant K represents the number of top predictions the model should return for a masked word. This involves ranking potential words that could replace the [MASK] token and selecting the top K candidates."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#the-main-function",
    "href": "projects/Bert/bertmodel.html#the-main-function",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "1.3 The main() function",
    "text": "1.3 The main() function\n\n\nCode\ndef main(text, viz_attentions = False):\n\n    # Tokenize input\n    tokenizer = BertTokenizer.from_pretrained(MODEL)\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    mask_token_index = get_mask_token_index(tokenizer.mask_token_id, inputs)\n    if mask_token_index is None:\n        sys.exit(f\"Input must include mask token {tokenizer.mask_token}.\")\n\n    # Use model to process input\n    model = BertForMaskedLM.from_pretrained(MODEL)\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation\n        result = model(**inputs, output_attentions=True)\n\n    # Generate predictions\n    mask_token_logits = result.logits[0, mask_token_index]\n    top_tokens = torch.topk(mask_token_logits, K).indices\n    for token in top_tokens:\n        print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n\n    # Visualize attentions\n    if viz_attentions:\n        visualize_attentions(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), result.attentions)\n\n\nNow, let’s briefly go over some important parts of the code and explain the underlying concepts:\nThe main() function:\nThis is where the script’s logic begins. The function must be called with a sentence containing a masked token ([MASK]). The masked token is crucial because BERT’s masked language modeling task involves predicting the masked word using the surrounding context.\nTokenization:\nThe input text is tokenized using the BERT tokenizer. Tokenization breaks the text into tokens (words or subwords) that the model can process. BERT uses WordPiece tokenization, which allows it to handle words and subwords. The argument return_tensors=\"pt\" ensures that the tokenized output is returned as PyTorch tensors, which are the primary data structures used by models in PyTorch. The get_mask_token_index() function finds the index of the [MASK] token in the tokenized input. This index is critical because the model predicts the missing word at this position.\nModel Inference:\nThe BERT model processes the tokenized inputs. It returns several outputs, including logits (which contain predictions for the masked token) and attentions (which contain the self-attention scores from each layer of the model). The output_attentions=True argument ensures that attention scores are included in the output.\nGenerating Predictions:\nThe script extracts the logits corresponding to the masked token. Logits are raw, unnormalized scores output by the model, representing its confidence in each possible token being the correct replacement for the [MASK] position. torch.topk is then used to select the top K highest-scoring tokens from the logits. These tokens are the model’s most likely candidates to replace the [MASK]. The top predicted tokens are decoded back into words or subwords using tokenizer.decode(). The original text is then printed with the [MASK] token replaced by each predicted token, showing what the model considers the most likely words in the given context.\nVisualizing Attention Scores:\nThe token IDs are converted back to their corresponding tokens (words or subwords) using tokenizer.convert_ids_to_tokens(). This is necessary because attention scores are tied to specific tokens, which need to be displayed in the visualization. The visualize_attentions() function is called to generate diagrams that represent the attention scores across different layers and heads in the BERT model."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#helper-functions",
    "href": "projects/Bert/bertmodel.html#helper-functions",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "1.4 Helper functions",
    "text": "1.4 Helper functions\nNow, let’s go over the helper functions that are used in the main() function.\n\n\nCode\ndef get_mask_token_index(mask_token_id, inputs):\n    \"\"\"\n    Return the index of the token with the specified `mask_token_id`, or\n    `None` if not present in the `inputs`.\n    \"\"\"\n    for i, token in enumerate(inputs['input_ids'][0]):\n        if token == mask_token_id:\n            return i\n    return None\n\n\nThe get_mask_token_index() function returns the index of the masked token in the input sequence. If the token is not found, it returns None.\n\n\nCode\ndef get_color_for_attention_score(attention_score):\n    \"\"\"\n    Return a tuple of three integers representing a shade of gray for the\n    given `attention_score`. Each value should be in the range [0, 255].\n    \"\"\"\n    return (round(attention_score * 255), round(attention_score * 255), round(attention_score * 255))\n\n\nThe get_color_for_attention_score() function converts an attention score (ranging from 0 to 1) into a grayscale color tuple, where higher scores correspond to lighter shades.\n\n\nCode\ndef generate_diagram(layer_number, head_number, tokens, attention_weights):\n    \"\"\"\n    Generate a diagram representing the self-attention scores for a single\n    attention head. The diagram shows one row and column for each of the\n    `tokens`, and cells are shaded based on `attention_weights`, with lighter\n    cells corresponding to higher attention scores.\n\n    The diagram is saved with a filename that includes both the `layer_number`\n    and `head_number`.\n    \"\"\"\n    # Create new image\n    image_size = GRID_SIZE * len(tokens) + PIXELS_PER_WORD\n    img = Image.new(\"RGBA\", (image_size, image_size), \"black\")\n    draw = ImageDraw.Draw(img)\n\n    # Draw each token onto the image\n    for i, token in enumerate(tokens):\n        # Draw token columns\n        token_image = Image.new(\"RGBA\", (image_size, image_size), (0, 0, 0, 0))\n        token_draw = ImageDraw.Draw(token_image)\n        token_draw.text(\n            (image_size - PIXELS_PER_WORD, PIXELS_PER_WORD + i * GRID_SIZE),\n            token,\n            fill=\"white\",\n            font=FONT\n        )\n        token_image = token_image.rotate(90)\n        img.paste(token_image, mask=token_image)\n\n        # Draw token rows\n        _, _, width, _ = draw.textbbox((0, 0), token, font=FONT)\n        draw.text(\n            (PIXELS_PER_WORD - width, PIXELS_PER_WORD + i * GRID_SIZE),\n            token,\n            fill=\"white\",\n            font=FONT\n        )\n\n    # Draw each word\n    for i in range(len(tokens)):\n        y = PIXELS_PER_WORD + i * GRID_SIZE\n        for j in range(len(tokens)):\n            x = PIXELS_PER_WORD + j * GRID_SIZE\n            color = get_color_for_attention_score(attention_weights[i][j].item())\n            draw.rectangle((x, y, x + GRID_SIZE, y + GRID_SIZE), fill=color)\n\n    # Save image in Attention images folder\n    Attention_folder_path = os.path.join(os.getcwd(), 'Attention images')\n    if not os.path.exists(Attention_folder_path):\n        os.makedirs(Attention_folder_path)\n\n    img.save(f\"{Attention_folder_path}/Attention_Layer{layer_number}_Head{head_number}.png\")\n\n\nThe generate_diagram() function generates an image visualizing the self-attention scores for a single attention head. It creates a grid where each cell is shaded based on the attention score between two tokens. The diagram is saved as an image file in the “Attention images” folder.\n\n\nCode\ndef visualize_attentions(tokens, attentions):\n    \"\"\"\n    Produce a graphical representation of self-attention scores.\n\n    For each attention layer, one diagram should be generated for each\n    attention head in the layer. Each diagram should include the list of\n    `tokens` in the sentence. The filename for each diagram should\n    include both the layer number (starting count from 1) and head number\n    (starting count from 1).\n    \"\"\"\n    # BERT's attentions are stored per-layer, per-head, and across the token sequence\n    for i, layer in enumerate(attentions):\n        # attentions are: batch_size x num_heads x seq_length x seq_length\n        for k, head_attention in enumerate(layer[0]):\n            layer_number = i + 1\n            head_number = k + 1\n            generate_diagram(\n                layer_number,\n                head_number,\n                tokens,\n                head_attention\n            )\n\n\nThe visualize_attentions() function orchestrates the generation of attention diagrams for all layers and heads of the BERT model. It iterates over the attention layers and heads, calling generate_diagram() to create and save an attention diagram for each head in each layer."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#layer-1-head-5",
    "href": "projects/Bert/bertmodel.html#layer-1-head-5",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "3.1 Layer 1, Head 5",
    "text": "3.1 Layer 1, Head 5\n\n\n\nAttention Layer1 Head5\n\n\nIn this diagram, we can see that the word ‘he’ is paying attention to the word ‘went’, which might indicate that this attention head is exploring the relationship between the subject and the verb."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#layer-5-head-1",
    "href": "projects/Bert/bertmodel.html#layer-5-head-1",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "3.2 Layer 5, Head 1",
    "text": "3.2 Layer 5, Head 1\n\n\n\nAttention Layer5 Head1\n\n\nHere we can see that the words ‘went’, ‘get’, and ‘some’ are attending to the word ‘supermarket’, which might indicate that the model is trying to understand the context of where the action is taking place, which is crucial for predicting what the person is going to get."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#layer-12-head-8",
    "href": "projects/Bert/bertmodel.html#layer-12-head-8",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "3.3 Layer 12, Head 8",
    "text": "3.3 Layer 12, Head 8\n\n\n\nAttention Layer12 Head8\n\n\nIn this case, the [MASK] token is attending to the words ‘supermarket,’ ‘get,’ and ‘some’, which might indicate that the model is trying to determine what the missing word should be based on the context of these words."
  },
  {
    "objectID": "projects/Bert/bertmodel.html#diagonal-patterns",
    "href": "projects/Bert/bertmodel.html#diagonal-patterns",
    "title": "Using a pre-trained BERT model for text prediction and analysing its self-attention scores",
    "section": "3.4 Diagonal patterns",
    "text": "3.4 Diagonal patterns\nThe following heads have show strong attention along the diagonal which indicate that they are focusing on local context and syntactic relationships, ensuring that each word’s immediate context is well-understood.\n\n3.4.1 Layer 3, Head 1\n\n\n\nAttention Layer3 Head1\n\n\nHere, we can see that each word is paying attention to the word that immediately follows it.\n\n\n3.4.2 Layer 3, Head 7\n\n\n\nAttention Layer3 Head7\n\n\nIn this case, each word is focusing on itself.\n\n\n3.4.3 Layer 4, Head 6\n\n\n\nAttention Layer4 Head6\n\n\nHere, we can see that each word is attending to the word that precedes it."
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html",
    "href": "projects/BnMclass/binaryNmulti.html",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "",
    "text": "In this project, we are going to explore how to use PyTorch to solve classification problems, specifically binary and multi-class classification problems. To achieve this, we will utilize the built-in datasets from the scikit-learn module and perform modeling using PyTorch.\nWe will follow these steps:\nLet’s first look at the binary classification problem, and then move on to the multi-class classification problem."
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#prepare-and-explore-the-data",
    "href": "projects/BnMclass/binaryNmulti.html#prepare-and-explore-the-data",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "1.1 Prepare and explore the data",
    "text": "1.1 Prepare and explore the data\nTo demonstrate the binary classification problem, we will use the make_circles() method from scikit-learn to generate two circles with differently colored dots.\n\n\nCode\n# Make 1000 samples\nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples, noise=0.03, random_state=42)\n\n\nNow, let’s explore our dataset.\n\n\nCode\n# Check the dimensions of our features and labels\n\nprint(f\"The dimension of feature X: {X.ndim}\")\nprint(f\"The dimension of label y: {y.ndim}\", end = '\\n\\n')\n\n# Check the shapes of our features and labels\n\nprint(f\"The shape of feature X: {X.shape}\")\nprint(f\"The shape of label y: {y.shape}\", end = '\\n\\n') # y is a scalar\n\n# View the first example of features and labels\n\nprint(f\"Values for FIRST sample of X: {X[0]} and the same for y: {y[0]}\")\nprint(f\"Shapes for FIRST sample of X: {X[0].shape} and the same for y: {y[0].shape}\", end = '\\n\\n')\n\n# Count no. of samples per label\n\nunique_values, counts = np.unique(y, return_counts=True)\nunique_counts_dict = {f'Label {val}': f'{count} samples' for val, count in zip(unique_values, counts)}\nprint(unique_counts_dict, end = '\\n\\n')\n\n# View first five samples\n\nprint(f\"First five X features:\\n{X[:5]}\")\nprint(f\"\\nFirst five corresponding y labels:\\n{y[:5]}\", end = '\\n\\n')\n\n\nThe dimension of feature X: 2\nThe dimension of label y: 1\n\nThe shape of feature X: (1000, 2)\nThe shape of label y: (1000,)\n\nValues for FIRST sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for FIRST sample of X: (2,) and the same for y: ()\n\n{'Label 0': '500 samples', 'Label 1': '500 samples'}\n\nFirst five X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst five corresponding y labels:\n[1 1 1 1 0]\nFrom the above results, we can see that there are 1000 samples of feature vector X with a dimension of 2 corresponding to 1000 samples of scalar label y with a dimension of 1. This means each pair of X features (say X1 and X2) has one corresponding label y, which is a scalar that is either 0 or 1. Therefore, we have two inputs for one output. Notice that we also have a balanced dataset, i.e., 500 samples per each label.\nNow let’s visualize the dataset.\n\n\nCode\n# Visualize dataset with a plot\n\nplt.figure(figsize=(5, 5))\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap=plt.cm.RdYlBu)\n\nplt.title(\"Circles Dataset\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\nNow, let’s find out how we could build a neural network using PyTorch to classify the dots into red (0) or blue (1), given X1 and X2 (x-axis and y-axis in the above plot). But, before proceeding to build the PyTorch model, we must first convert our data into tensors and split it into 80% training and 20% testing datasets.\n\n\nCode\n# Turn the data into tensors\n\nX = torch.tensor(X, dtype=torch.float)\ny = torch.tensor(y, dtype=torch.float)\n\n\n\n\nCode\n# Split data into train and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Number of training and testing samples\n\nprint(f\"Number of training samples: {len(X_train)}\")\nprint(f\"Number of testing samples: {len(X_test)}\")\n\n\nNumber of training samples: 800\nNumber of testing samples: 200"
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#build-the-model",
    "href": "projects/BnMclass/binaryNmulti.html#build-the-model",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "1.2 Build the model",
    "text": "1.2 Build the model\nNow that we have our data ready, let’s build our model using PyTorch.\nSince our data is not linearly separable, we will use a non-linear activation function such as ReLU in the model.\n\n\nCode\n# Construct the model\n\nclass CircleModel(nn.Module):\n\n    def __init__(self, hidden_units=10):\n        super().__init__()\n        self.layer_1 = nn.Linear(2, hidden_units)\n        self.layer_2 = nn.Linear(hidden_units, hidden_units)\n        self.layer_3 = nn.Linear(hidden_units, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer_1(x))\n        x = self.relu(self.layer_2(x))\n        return self.layer_3(x)"
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#train-and-evaluate-the-model",
    "href": "projects/BnMclass/binaryNmulti.html#train-and-evaluate-the-model",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "1.3 Train and evaluate the model",
    "text": "1.3 Train and evaluate the model\nNow, let’s define train_and_evaluate() function to train and evaluate the model. This function trains the model for a specified number of epochs, computes training and testing loss and accuracy, and prints the results every 100 epochs.\nThe PyTorch training loop typically contains the following steps:\n\nForward pass - The model takes X_train as input, performs forward() function calculations, and then outputs raw prediction scores (logits) (y_logits = model(X_train).squeeze()). These logits are then converted into prediction probabilities by the sigmoid activation function. Finally, we round the probabilities to 0 or 1 according to the set threshold (here we use 0.5) to get binary predictions (y_pred = torch.round(torch.sigmoid(y_logits)).\nCalculate the loss - The model’s outputs (predictions) are compared to the actual values and evaluated to see how wrong they are (loss_fn(y_pred, y_train)).\nZero the gradients - Clears the gradients of all model parameters, resetting them to zero. This is necessary because gradients accumulate by default in PyTorch (optimizer.zero_grad()).\nPerform backpropagation - Performs backpropagation, calculating the gradient of the loss with respect to each model parameter (loss.backward()).\nStep the optimizer (gradient descent) - Updates the model parameters using the gradients computed in the backward pass (optimizer.step()).\n\nIn the evaluation loop, we use testing data to evaluate the performance of the model by computing the model’s predictions (logits) on the test data, converting the logits to probabilities, and then to binary predictions, similar to the training loop.\n\n\nCode\ndef train_and_evaluate(model, X_train, y_train, X_test, y_test, loss_fn, optimizer, epochs, device='cpu'):\n\n    # Ensure the model is on the correct device\n    model.to(device)\n\n    # Move data to the device\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    X_test, y_test = X_test.to(device), y_test.to(device)\n\n    for epoch in range(epochs):\n\n        ## Training\n\n        # Forward pass\n        y_logits = model(X_train).squeeze()\n        y_pred = torch.round(torch.sigmoid(y_logits))\n\n        # Calculate loss and accuracy\n        loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n        acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n\n        # Optimizer zero grad\n        optimizer.zero_grad()\n\n        # Calculate the Loss\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n\n        ## Testing\n\n        model.eval()\n        with torch.no_grad():\n            # Forward pass\n            test_logits = model(X_test).squeeze()\n            test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n\n            # Calculate loss and accuracy\n            test_loss = loss_fn(test_logits, y_test)\n            test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n\n        # Print out what's happening\n        if epoch % 100 == 0:\n            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n\n\n\n1.3.1 Define accuracy function\nSince our dataset is a balanced dataset, in addition to the loss evaluation metric, we’ll also use the accuracy evaluation metric. Accuracy measures the proportion of correctly classified samples out of the total number of samples. We define a function called accuracy_fn() to perform this task.\n\n\nCode\n# Calculate accuracy\n\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item()\n    return (correct / len(y_pred)) * 100\n\n\n\n\n1.3.2 Setup loss and optimizer\nSince we are doing binary classification, we can either use torch.nn.BCELossWithLogits or torch.nn.BCELoss as our loss function. Let’s use torch.nn.BCELossWithLogits for this simple problem as it has a built-in sigmoid activation and is also more numerically stable than torch.nn.BCELoss. As for the optimizer, let’s use torch.optim.SGD() to optimize the model parameters with a learning rate of 0.1.\nLet’s first set up device-agnostic code and then instantiate the model.\n\n\nCode\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate the model\nmodel = CircleModel().to(device)\n\n# Setup loss function\nloss_fn = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss includes sigmoid\n\n# Setup optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n\nNow let’s train and evaluate the model using the above-defined train_and_evaluate() function for 1000 epochs.\n\n\nCode\ntrain_and_evaluate(model, X_train, y_train, X_test, y_test, loss_fn, optimizer, epochs=1000, device=device)\n\n\nEpoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\nIdeally, the loss should steadily decrease to 0 while accuracy increases to 100%. However, we can see that although accuracy is steadily increasing, the loss remains almost stagnant. This may be due to the following reasons:\nDifference between loss function and accuracy metrics:\nLoss functions and accuracy metrics can respond differently to changes in predictions. For example, we are using nn.BCEWithLogitsLoss, which measures the difference between the predicted probabilities and the true labels, while accuracy simply counts the proportion of correct predictions. Accuracy is a more straightforward metric, reflecting whether predictions match the labels. In contrast, loss considers how confident the predictions are and penalizes incorrect predictions more if they are made with high confidence.\nPrediction probabilities and thresholding:\nTo classify the data, our model is using a threshold of 0.5 to convert prediction probabilities into binary predictions. Therefore, accuracy may appear high if the model’s predictions are often very close to the threshold. However, the loss function still evaluates based on the exact probability values, which might not improve if the model’s confidence is misplaced.\nLet’s plot the decision boundary of the model using both the train and test data to understand how well our model has learned.\n\n\n1.3.3 Visualizing the decision boundary\nA decision boundary is a line or a surface that separates different classes in a dataset. We’ll define plot_decision_boundary() function to plot the decision boundary for our dataset. It creates a grid of points covering the feature space, passes these points through the model to get predictions, and then plots these predictions along with the actual data points.\n\n\nCode\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n\n    \"\"\"Plot the decision boundary of a model's predictions on X against y.\"\"\"\n\n    # Ensure model and data are on CPU\n    device = torch.device(\"cpu\")\n    model.to(device)\n    X, y = X.to(device), y.to(device)\n\n    # Set up grid boundaries for plotting\n    x_min, x_max = X[:, 0].min().item() - 0.1, X[:, 0].max().item() + 0.1\n    y_min, y_max = X[:, 1].min().item() - 0.1, X[:, 1].max().item() + 0.1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 101),\n        np.linspace(y_min, y_max, 101)\n    )\n\n    # Generate predictions for grid points\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float().to(device)\n\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Determine prediction labels based on problem type (binary vs. multi-class)\n    y_pred = (\n        torch.softmax(y_logits, dim=1).argmax(dim=1)\n        if y.unique().numel() &gt; 2\n        else torch.sigmoid(y_logits).round()\n    )\n\n    # Reshape predictions and plot the decision boundary\n    y_pred = y_pred.reshape(xx.shape).cpu().numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0].cpu(), X[:, 1].cpu(), c=y.cpu(), s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n\n\n\nCode\n# Plot decision boundaries for train set\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Train data\")\nplot_decision_boundary(model, X_train, y_train)\nplt.show()\n\n\n\n\n\nCode\n# Plot decision boundaries for test set\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Test data\")\nplot_decision_boundary(model, X_test, y_test)\nplt.show()\n\n\n\nFrom the above diagrams, we can see that our model is underfitting, but not to the extent suggested by the loss value. For a loss value of around 0.6, we would expect about half of the points to be misclassified, but that’s not the case here. Therefore, the accuracy metric is more appropriate for evaluating the model."
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#improve-the-model",
    "href": "projects/BnMclass/binaryNmulti.html#improve-the-model",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "1.4 Improve the model",
    "text": "1.4 Improve the model\nSince our model is underfitting, we can improve the model in several ways, such as experimenting with different architectures, activation functions, or optimizers etc. As we have a simple problem to solve, let’s train it for another 1000 epochs and see what happens.\n\n\nCode\ntrain_and_evaluate(model, X_train, y_train, X_test, y_test, loss_fn, optimizer, epochs=1000, device=device)\n\n\nEpoch: 0 | Loss: 0.56818, Accuracy: 87.75% | Test Loss: 0.57378, Test Accuracy: 86.50%\nEpoch: 100 | Loss: 0.48153, Accuracy: 93.50% | Test Loss: 0.49935, Test Accuracy: 90.50%\nEpoch: 200 | Loss: 0.37056, Accuracy: 97.75% | Test Loss: 0.40595, Test Accuracy: 92.00%\nEpoch: 300 | Loss: 0.25458, Accuracy: 99.00% | Test Loss: 0.30333, Test Accuracy: 96.50%\nEpoch: 400 | Loss: 0.17180, Accuracy: 99.50% | Test Loss: 0.22108, Test Accuracy: 97.50%\nEpoch: 500 | Loss: 0.12188, Accuracy: 99.62% | Test Loss: 0.16512, Test Accuracy: 99.00%\nEpoch: 600 | Loss: 0.09123, Accuracy: 99.88% | Test Loss: 0.12741, Test Accuracy: 99.50%\nEpoch: 700 | Loss: 0.07100, Accuracy: 99.88% | Test Loss: 0.10319, Test Accuracy: 99.50%\nEpoch: 800 | Loss: 0.05773, Accuracy: 99.88% | Test Loss: 0.08672, Test Accuracy: 99.50%\nEpoch: 900 | Loss: 0.04853, Accuracy: 99.88% | Test Loss: 0.07474, Test Accuracy: 99.50%\nLooks like our model has converged and we got the results what we wanted i.e. loss close to zero and accuracy close to 100%. Now, let’s visualize the decision boundary to see what’s hapenning.\n\n\nCode\n# Plot decision boundaries for test set\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Model Test\")\nplot_decision_boundary(model, X_test, y_test)\nplt.show()\n\n\n\nThe decision boundary plot also confirms that our model has improved.\nNow that we have done with our binary classification, let’s move on to multi-class classification problem."
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#prepare-and-explore-the-data-1",
    "href": "projects/BnMclass/binaryNmulti.html#prepare-and-explore-the-data-1",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "2.1 Prepare and explore the data",
    "text": "2.1 Prepare and explore the data\nFirst, let’s import the required modules. We are re-importing most of the modules for the sake of completeness.\n\n\nCode\n# Import the required modules\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\n\n\nNow, we will use the make_blobs() method from scikit-learn to generate isotropic Gaussian blobs.\n\n\nCode\n# Set the hyperparameters for data creation\n\nNUM_SAMPLES = 1000\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# Create multi-class data\n\nX_blob, y_blob = make_blobs(n_samples=NUM_SAMPLES,\n                            n_features=NUM_FEATURES, # X features\n                            centers=NUM_CLASSES, # y labels\n                            cluster_std=2, # standard deviation of the cluster\n                            random_state=RANDOM_SEED\n                            )\n\n# Count no. of samples per label\n\nunique_values, counts = np.unique(y_blob, return_counts=True)\nunique_counts_dict = {f'Label {val}': f'{count} samples' for val, count in zip(unique_values, counts)}\nprint(unique_counts_dict, end = '\\n\\n')\n\n# View ten samples\n\nprint(f\"Ten 'X' features:\\n{X_blob[-10:]}\")\nprint(f\"\\n Ten corresponding 'y' labels:\\n{y_blob[-10:]}\", end = '\\n\\n')\n\n\n{'Label 0': '250 samples', 'Label 1': '250 samples', 'Label 2': '250 samples', 'Label 3': '250 samples'}\n\nten X features:\n[[ 4.17800978  3.36558241]\n [-4.18864131  7.81550084]\n [ 5.25548237 -1.4471671 ]\n [-9.00985452 -7.490559  ]\n [ 6.93842549  0.56681683]\n [-2.75662004 -4.46337713]\n [-1.26095799 10.27097715]\n [ 2.74108106  7.23793381]\n [-8.0986516  -7.2540522 ]\n [-9.96266381  6.90507917]]\n\n ten corresponding y labels:\n[1 0 1 2 1 2 0 1 2 3]\nThe above make_blobs() function generates a balanced dataset with 1,000 samples, 2 features, 4 centers (clusters), and a specified spread within each cluster (cluster_std=2). The random_state parameter ensures that the data is the same every time it’s generated.\nNow, we will visualize our dataset.\n\n\nCode\n# Plot the dataset\n\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)\nplt.show()\n\n\n\nNow, let’s prepare our data for modeling with PyTorch.\n\n\nCode\n# Turn data into tensors\nX_blob = torch.tensor(X_blob, dtype=torch.float)\ny_blob = torch.tensor(y_blob, dtype=torch.long)\n\n\n\n\nCode\n# Split the data into train and test sets\n\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n\n# Number of training and testing samples\n\nprint(f\"Number of training samples: {len(X_blob_train)}\")\nprint(f\"Number of testing samples: {len(X_blob_test)}\")\n\n\nNumber of training samples: 800\nNumber of testing samples: 200"
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#build-the-model-1",
    "href": "projects/BnMclass/binaryNmulti.html#build-the-model-1",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "2.2 Build the model",
    "text": "2.2 Build the model\nNow that we have our data ready, we will build our model using PyTorch.\nSince our data appears to be linearly separable in the above plot, let’s use only linear layers in the model and see if it works.\n\n\nCode\nclass BlobModel(nn.Module):\n\n    def __init__(self, input_features, output_features, hidden_units=8):\n        super(BlobModel, self).__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(input_features, hidden_units),\n            nn.Linear(hidden_units, hidden_units),\n            nn.Linear(hidden_units, output_features)\n        )\n\n    def forward(self, x):\n        return self.linear_layer_stack(x)"
  },
  {
    "objectID": "projects/BnMclass/binaryNmulti.html#train-and-evaluate-the-model-1",
    "href": "projects/BnMclass/binaryNmulti.html#train-and-evaluate-the-model-1",
    "title": "Developing neural network models for classification problems using PyTorch",
    "section": "2.3 Train and evaluate the model",
    "text": "2.3 Train and evaluate the model\nNow that we have our model ready, we will define the train_and_evaluate() function similarly to how we defined it for binary classification. This function will train and evaluate the model, running for a specified number of epochs, computing training and testing loss and accuracy, and printing the results every 100 epochs.\nWe have already discussed the steps involved in training and evaluating PyTorch models in the binary classification problem. For multi-class classification, we need to adjust the function to accommodate the specifics of this problem, such as the forward pass and the use of different loss functions and optimizers.\nIn the forward pass, the model outputs logits (raw predictions), which are then converted to predicted labels using the argmax() function.\nHere, we’ll use the nn.CrossEntropyLoss() method as our loss function. For the optimizer, we will use torch.optim.SGD() to optimize the model parameters with a learning rate of 0.1.\nAdditionally, we will use the previously defined accuracy_fn() function to evaluate accuracy.\n\n\nCode\ndef train_and_evaluate(model, X_blob_train, y_blob_train, X_blob_test, y_blob_test, loss_fn, optimizer, epochs=1000, device='cpu'):\n\n    # Ensure the model is on the correct device\n    model.to(device)\n\n    # Move data to the device\n    X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n    X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\n    for epoch in range(epochs):\n\n        ## Trainig\n        model.train()\n\n        # Forward pass\n        y_logits = model(X_blob_train)\n        #y_pred = y_logits.argmax(dim=1)\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n\n        # Calculate loss and accuracy\n        loss = loss_fn(y_logits, y_blob_train)\n        acc = accuracy_fn(y_blob_train, y_pred)\n\n        # Optimizer zero grad\n        optimizer.zero_grad()\n\n        # Loss backward\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n\n        ## Testing\n        model.eval()\n        with torch.no_grad():\n            test_logits = model(X_blob_test)\n            #test_pred = test_logits.argmax(dim=1)\n            test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n\n            # Calculate test loss and accuracy\n            test_loss = loss_fn(test_logits, y_blob_test)\n            test_acc = accuracy_fn(y_blob_test, test_pred)\n\n        # Print out what's happening\n        if epoch % 100 == 0:\n            print(f\"Epoch: {epoch} | Train Loss: {loss:.5f}, Train Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n\n\n\n2.3.1 Setup loss and optimizer\n\n\nCode\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize model, loss function, and optimizer\nmodel = BlobModel(input_features=NUM_FEATURES, output_features=NUM_CLASSES, hidden_units=8)\n\n# Setup loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Setup optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n\nNow we will train and evaluate the model using the train_and_evaluate() function for 1000 epochs.\n\n\nCode\ntrain_and_evaluate(model, X_blob_train, y_blob_train, X_blob_test, y_blob_test, loss_fn, optimizer, epochs=1000, device=device)\n\n\nEpoch: 0 | Train Loss: 1.05193, Train Accuracy: 63.00% | Test Loss: 0.59764, Test Accuracy: 91.50%\nEpoch: 100 | Train Loss: 0.09545, Train Accuracy: 96.25% | Test Loss: 0.08077, Test Accuracy: 96.50%\nEpoch: 200 | Train Loss: 0.09102, Train Accuracy: 96.25% | Test Loss: 0.07636, Test Accuracy: 96.50%\nEpoch: 300 | Train Loss: 0.08858, Train Accuracy: 96.25% | Test Loss: 0.07355, Test Accuracy: 96.50%\nEpoch: 400 | Train Loss: 0.08655, Train Accuracy: 96.38% | Test Loss: 0.07098, Test Accuracy: 96.50%\nEpoch: 500 | Train Loss: 0.08469, Train Accuracy: 96.38% | Test Loss: 0.06858, Test Accuracy: 96.50%\nEpoch: 600 | Train Loss: 0.08296, Train Accuracy: 96.38% | Test Loss: 0.06633, Test Accuracy: 97.00%\nEpoch: 700 | Train Loss: 0.08133, Train Accuracy: 96.62% | Test Loss: 0.06422, Test Accuracy: 97.00%\nEpoch: 800 | Train Loss: 0.07980, Train Accuracy: 96.62% | Test Loss: 0.06224, Test Accuracy: 97.00%\nEpoch: 900 | Train Loss: 0.07844, Train Accuracy: 96.62% | Test Loss: 0.05917, Test Accuracy: 97.50%\n\n\n2.3.2 Visualizing the decision boundary\nHere we will use the previously defined plot_decision_boundary() function to plot the decision boundaries for our model.\n\n\nCode\n# Plot decision boundaries\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train data\")\nplot_decision_boundary(model, X_blob_train, y_blob_train)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Test data\")\nplot_decision_boundary(model, X_blob_test, y_blob_test)\nplt.show()\n\n\n\nFrom the results above, we can see that our model has nearly 0 loss and 97% accuracy. This indicates that it is performing quite well, and there may be no need for further improvements."
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html",
    "href": "projects/finetuningBert/finetuningBert.html",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "",
    "text": "This project involves:"
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html#imdb-dataset",
    "href": "projects/finetuningBert/finetuningBert.html#imdb-dataset",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "IMDb dataset",
    "text": "IMDb dataset\nWe will use the IMDb dataset from Hugging Face’s datasets library to fine-tune our model for binary sentiment classification. The dataset consists of movie reviews labeled as either positive or negative. The original dataset has balanced train and test datasets, each containing 25,000 labeled samples (reviews) and an additional 50,000 unlabeled samples for unsupervised learning.\nlet’s load and explore the original IMDb dataset.\n\n\nCode\nfrom datasets import load_dataset, concatenate_datasets\n\n# Load the IMDb dataset\nimdb_dataset = load_dataset(\"imdb\")\n\n\n\n\nCode\n# Print the new imdb dataset\n\nprint(imdb_dataset)\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n\n\nCode\n# Inspecting the dataset structure\n\nprint(imdb_dataset[\"train\"].features)\n\n\n{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n\n\nCode\n# Checking the first review to see what it looks like\n\nprint(imdb_dataset[\"train\"][0][\"text\"])\n\n\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n\n\nCode\n# Checking the label of the first review\n\nprint(imdb_dataset[\"train\"][0][\"label\"])\n\n\n0"
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html#creating-a-smaller-dataset",
    "href": "projects/finetuningBert/finetuningBert.html#creating-a-smaller-dataset",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "Creating a Smaller Dataset",
    "text": "Creating a Smaller Dataset\nHere, we will only use a subset of the IMDb dataset to reduce the computational requirements. (as training the original full dataset alone on Google Colab’s T4 GPU previously took me approximately 40 minutes).\nSo, we will create a new IMDb dataset consisting of a balanced training set with 8000 samples (reviews) and a test set with 2000 samples, both drawn from the original IMDb dataset.\n\n\nCode\n# Define a function to balance the dataset\ndef balance_dataset(dataset, label_col, num_samples):\n    # Filter positive and negative examples\n    positive_samples = dataset.filter(lambda example: example[label_col] == 1)\n    negative_samples = dataset.filter(lambda example: example[label_col] == 0)\n\n    # Subsample both to the desired number\n    positive_samples = positive_samples.shuffle(seed=42).select(range(num_samples // 2))\n    negative_samples = negative_samples.shuffle(seed=42).select(range(num_samples // 2))\n\n    # Concatenate positive and negative examples to form a balanced dataset\n    balanced_dataset = concatenate_datasets([positive_samples, negative_samples]).shuffle(seed=42)\n\n    return balanced_dataset\n\n\n\n\nCode\n# Create a balanced train and test dataset\ntrain_data = balance_dataset(imdb_dataset[\"train\"], \"label\", 8000)\ntest_data = balance_dataset(imdb_dataset[\"test\"], \"label\", 2000)\n\n# Checking the datasets\nprint(f\"train_data:\\n {train_data}\", end='\\n\\n')\nprint(f\"test_data:\\n {test_data}\")\n\n\ntrain_data:\n Dataset({\n    features: ['text', 'label'],\n    num_rows: 8000\n})\n\ntest_data:\n Dataset({\n    features: ['text', 'label'],\n    num_rows: 2000\n})"
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html#train-and-evaluate-the-model",
    "href": "projects/finetuningBert/finetuningBert.html#train-and-evaluate-the-model",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "1.1 Train and Evaluate the model",
    "text": "1.1 Train and Evaluate the model\nNow, we define the hyperparameters that we want to tune in the TrainingArguments class:\n\nlearning_rate=2e-5: The learning rate used for updating model parameters.\nper_device_train_batch_size=16 and per_device_eval_batch_size=16: Batch size during training and evaluation.\nnum_train_epochs=1: The number of epochs (complete passes through the dataset).\nweight_decay=0.01: A regularization term to prevent overfitting.\nsave_strategy=\"epoch\": The model will be saved at the end of each epoch.\nreport_to=\"none\": Disables reporting to external logging services.\n\n\n\nCode\nfrom transformers import TrainingArguments, Trainer\n\n# Training arguments for parameter tuning\ntraining_args = TrainingArguments(\n   \"model\",\n   learning_rate=2e-5,\n   per_device_train_batch_size=16,\n   per_device_eval_batch_size=16,\n   num_train_epochs=1,\n   weight_decay=0.01,\n   save_strategy=\"epoch\",\n   report_to=\"none\"\n)\n\n# Trainer which executes the training process\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_train,\n   eval_dataset=tokenized_test,\n   tokenizer=tokenizer,\n   data_collator=data_collator,\n   compute_metrics=compute_metrics,\n)\n\n\nWe will perform model training and evaluation using the Trainer class.\n\n\nCode\ntrainer.train()\n\n\nTrainOutput(global_step=500, training_loss=0.31193878173828127, metrics={'train_runtime': 742.959, 'train_samples_per_second': 10.768, 'train_steps_per_second': 0.673, 'total_flos': 2084785113806400.0, 'train_loss': 0.31193878173828127, 'epoch': 1.0})\nThe output of trainer.evaluate() will be a dictionary containing default metrics, along with the F1 score metric that we defined as part of the compute_metrics() function (since F1 was specified as the custom evaluation metric).\n\n\nCode\ntrainer.evaluate()\n\n\n{'eval_loss': 0.19807493686676025,\n 'eval_f1': 0.9257425742574258,\n 'eval_runtime': 59.8717,\n 'eval_samples_per_second': 33.405,\n 'eval_steps_per_second': 2.088,\n 'epoch': 1.0}\nFrom the results, we can observe that the training time for fine-tuning is approximately 742.95 seconds, and we achieved an F1 score of 0.925"
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html#train-and-evalute-the-partially-frozen-layer-model",
    "href": "projects/finetuningBert/finetuningBert.html#train-and-evalute-the-partially-frozen-layer-model",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "2.1 Train and Evalute the partially frozen layer model",
    "text": "2.1 Train and Evalute the partially frozen layer model\n\n\nCode\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_train,\n   eval_dataset=tokenized_test,\n   tokenizer=tokenizer,\n   data_collator=data_collator,\n   compute_metrics=compute_metrics,\n)\n\n\ntrainer.train()\n\n\nTrainOutput(global_step=500, training_loss=0.389836181640625, metrics={'train_runtime': 322.8201, 'train_samples_per_second': 24.782, 'train_steps_per_second': 1.549, 'total_flos': 2084785113806400.0, 'train_loss': 0.389836181640625, 'epoch': 1.0})\n\n\nCode\ntrainer.evaluate()\n\n\n{'eval_loss': 0.244190514087677,\n 'eval_f1': 0.9073610415623435,\n 'eval_runtime': 59.2922,\n 'eval_samples_per_second': 33.731,\n 'eval_steps_per_second': 2.108,\n 'epoch': 1.0}\nWe observed that the training time was reduced to 322.82 seconds, more than half the time of the fully fine-tuned model, while maintaining good performance with an F1 score of 0.90"
  },
  {
    "objectID": "projects/finetuningBert/finetuningBert.html#train-and-evalute-the-completely-frozen-bert-model",
    "href": "projects/finetuningBert/finetuningBert.html#train-and-evalute-the-completely-frozen-bert-model",
    "title": "Fine tuning a Representation Model for Binary Sentiment Classification",
    "section": "3.1 Train and Evalute the completely frozen BERT model",
    "text": "3.1 Train and Evalute the completely frozen BERT model\n\n\nCode\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_train,\n   eval_dataset=tokenized_test,\n   tokenizer=tokenizer,\n   data_collator=data_collator,\n   compute_metrics=compute_metrics,\n)\n\n\ntrainer.train()\n\n\nTrainOutput(global_step=500, training_loss=0.6961300659179688, metrics={'train_runtime': 249.3582, 'train_samples_per_second': 32.082, 'train_steps_per_second': 2.005, 'total_flos': 2084785113806400.0, 'train_loss': 0.6961300659179688, 'epoch': 1.0})\n\n\nCode\ntrainer.evaluate()\n\n\n{'eval_loss': 0.6841261982917786,\n 'eval_f1': 0.5947006869479883,\n 'eval_runtime': 59.5484,\n 'eval_samples_per_second': 33.586,\n 'eval_steps_per_second': 2.099,\n 'epoch': 1.0}\nAs expected, the training time is the shortest (249.35 seconds) in this scenario, but the model’s F1 score is only 0.59, the worst performance among all the experiments, as no fine-tuning was done in the main BERT layers."
  },
  {
    "objectID": "projects/NimGame/NimGame.html",
    "href": "projects/NimGame/NimGame.html",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "",
    "text": "In this project, we will build an AI to learn the strategy for playing the game Nim through reinforcement learning. In particular, we will explore one of the models of reinforcement learning called Q-learning. But first, let’s look at how the Nim game is played."
  },
  {
    "objectID": "projects/NimGame/NimGame.html#defining-the-nim-game",
    "href": "projects/NimGame/NimGame.html#defining-the-nim-game",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "3.1 Defining the Nim game",
    "text": "3.1 Defining the Nim game\nThe Nim class defines how a Nim game is played. An object of the Nim class is initialized to keep track of the piles, the current player, and the winner of the game. The available_actions method returns a set of all the available actions for a given state. The move method checks the validity of a player’s move/action, updates the number of objects in the piles according to the move, switches the player’s turn using the switch_player and other_player methods, and finally checks for a winner.\n\n\nCode\n# Import the required modules\n\nimport math\nimport random\nimport time\n\n\nclass Nim():\n    \"\"\" Class to define the Nim game itself \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize game board.\n        Each game board has\n            - `piles`: a list of how many objects remain in each pile\n            - `player`: 0 or 1 to indicate which player's turn\n            - `winner`: None, 0, or 1 to indicate who the winner is\n        \"\"\"\n        initial=[1, 3, 5, 7]\n        self.piles = initial.copy()\n        self.player = 0\n        self.winner = None\n\n    @classmethod\n    def available_actions(cls, piles):\n        \"\"\"\n        Nim.available_actions(piles) takes a `piles` list aka `state` as input\n        and returns all of the available actions `(i, j)` in that state.\n\n        Action `(i, j)` represents the action of removing `j` items\n        from pile `i` (where piles are 0-indexed).\n        \"\"\"\n        actions = set()\n        for i, pile in enumerate(piles):\n            for j in range(1, pile + 1):\n                actions.add((i, j))\n        return actions\n\n    @classmethod\n    def other_player(cls, player):\n        \"\"\"\n        Nim.other_player(player) returns the player that is not\n        `player`. Assumes `player` is either 0 or 1.\n        \"\"\"\n        return 0 if player == 1 else 1\n\n    def switch_player(self):\n        \"\"\"\n        Switch the current player to the other player.\n        \"\"\"\n        self.player = Nim.other_player(self.player)\n\n    def move(self, action):\n        \"\"\"\n        Make the move aka `action` for the current player.\n        `action` must be a tuple `(i, j)`.\n        \"\"\"\n        pile, count = action\n\n        # Check for the validity of the move\n        if self.winner is not None:\n            raise Exception(\"Game already won\")\n        elif pile &lt; 0 or pile &gt;= len(self.piles):\n            raise Exception(\"Invalid pile\")\n        elif count &lt; 1 or count &gt; self.piles[pile]:\n            raise Exception(\"Invalid number of objects\")\n\n        # Update pile\n        self.piles[pile] -= count\n        \n        # Switch the player\n        self.switch_player()\n\n        # Check for a winner\n        if all(pile == 0 for pile in self.piles):\n            self.winner = self.player"
  },
  {
    "objectID": "projects/NimGame/NimGame.html#defining-the-nimai",
    "href": "projects/NimGame/NimGame.html#defining-the-nimai",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "3.2 Defining the NimAI",
    "text": "3.2 Defining the NimAI\nThe NimAI class defines the AI where the Q-learning algorithm is implemented. An object of the NimAI class is initialized with an empty q_value dictionary, and default values of 0.5 and 0.1 for alpha and epsilon respectively. The q_value dictionary keeps track of all Q-values of (state, action) pairs learned by the AI. Alpha is used in the Q-learning formula, and epsilon is used for action selection.\nThe update method updates the Q-value of an action in a state. It takes inputs: old_state (the state where the action is taken), action (the action taken in the old state), reward (the immediate reward for that action in that state), and new_state (the state resulting from taking that action). The method then implements Q-learning by retrieving the current Q-value using get_q_value, determining the best possible future rewards with best_future_reward, and updating the Q-value using update_q_value.\nLastly, the choose_action method selects an action for a given state based on an epsilon-greedy algorithm. The epsilon-greedy algorithm allows the AI to explore other actions in a state to maximize future rewards. With probability epsilon, the AI chooses a random action (explore), and with probability (1 - epsilon), it chooses the action with the highest Q-value (exploit). This balance between exploration and exploitation helps in improving the AI’s performance over time.\n\n\nCode\nclass NimAI():\n    \"\"\" Class to define the AI \"\"\"\n    def __init__(self, alpha=0.5, epsilon=0.1):\n        \"\"\"\n        Initialize AI with an empty Q-learning dictionary,\n        an alpha (learning) rate, and an epsilon rate.\n\n        The Q-learning dictionary maps `(state, action)` pairs to a Q-value (a number).\n         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n         - `action` is a tuple `(i, j)` for an action\n        \"\"\"\n        self.q_value = {}\n        self.alpha = alpha\n        self.epsilon = epsilon\n\n    def update(self, old_state, action, new_state, reward):\n        \"\"\"\n        Update Q-learning model, given an old state, an action taken\n        in that state, a new resulting state, and the reward received\n        from taking that action.\n        \"\"\"\n        old = self.get_q_value(old_state, action)\n        best_future = self.best_future_reward(new_state)\n        self.update_q_value(old_state, action, old, reward, best_future)\n\n    def get_q_value(self, state, action):\n        \"\"\"\n        Return the Q-value for the state `state` and the action `action`.\n        If no Q-value exists yet in `self.q`, return 0.\n        \"\"\"\n        # If there is a Q-value for current (state, action)\n        # already in `self.q`, return it\n        if (tuple(state), action) in self.q_value:\n            return self.q_value[(tuple(state), action)]\n\n        # If current (state action) is not explored yet, Q-value is 0\n        return 0\n\n    def update_q_value(self, state, action, old_q, reward, future_rewards):\n        \"\"\"\n        Update the Q-value for the state `state` and the action `action`\n        given the previous Q-value `old_q`, a current reward `reward`,\n        and an estiamte of future rewards `future_rewards`.\n\n        Use the formula:\n\n        Q(s, a) &lt;- old value estimate + alpha * (new value estimate - old value estimate)\n\n        where `old value estimate` is the previous Q-value,\n        `alpha` is the learning rate, and `new value estimate`\n        is the sum of the current reward and estimated future rewards.\n        \"\"\"\n        # Calculate and get constants\n        new_value_estimate = reward + future_rewards\n        alpha = self.alpha\n\n        # Update Q-value according to the formula\n        self.q_value[(tuple(state), action)] = old_q + alpha * (new_value_estimate - old_q)\n\n    def best_future_reward(self, state):\n        \"\"\"\n        Given a state `state`, consider all possible `(state, action)`\n        pairs available in that state and return the maximum of all\n        of their Q-values.\n\n        Use 0 as the Q-value if a `(state, action)` pair has no\n        Q-value in `self.q`. If there are no available actions in\n        `state`, return 0.\n        \"\"\"\n        possible_actions = Nim.available_actions(state)\n\n        # Corner case where there is no possible actions\n        if len(possible_actions) == 0:\n            return 0\n\n        # Initialize cur_reward to as low as possible to make sure\n        # There are better actions than None\n        reward = -math.inf\n\n        for action in possible_actions:\n            # If action is in self.q and the reward of the action \n            # is better than current reward, update reward\n            cur_reward = self.get_q_value(state, action)\n            if cur_reward &gt; reward:\n                reward = cur_reward\n\n        return reward\n\n    def choose_action(self, state, epsilon=True):\n        \"\"\"\n        Given a state `state`, return an action `(i, j)` to take.\n\n        If `epsilon` is `False`, then return the best action\n        available in the state (the one with the highest Q-value,\n        using 0 for pairs that have no Q-values).\n\n        If `epsilon` is `True`, then with probability\n        `self.epsilon` choose a random available action,\n        otherwise choose the best action available.\n\n        If multiple actions have the same Q-value, any of those\n        options is an acceptable return value.\n        \"\"\"\n        # Initialize all possible actions\n        # Set highest Q-value to as low as possible to make sure\n        # some action has better  Q-value than the initial best action of None\n        possible_actions = Nim.available_actions(state)\n        highest_q = -math.inf \n        best_action = None    \n\n        # Iterate all possible actions and compare the Q-value of each\n        for action in possible_actions:\n            current_q = self.get_q_value(state, action)\n            # If current action is better, update current Q-value and best_action\n            if current_q &gt; highest_q:\n                highest_q = current_q\n                best_action = action\n\n        # If epsilon is true, take random action according to probabilities\n        # Exploration vs Exploitation\n        if epsilon:\n            # For self.epsilon chance, take random action\n            # For 1 - self.epsilon chance, take best action\n            action_weights = [self.epsilon / len(possible_actions) if action != best_action else\n                                (1 - self.epsilon) for action in possible_actions]\n\n            best_action = random.choices(list(possible_actions), weights=action_weights, k=1)[0]\n\n        return best_action"
  },
  {
    "objectID": "projects/NimGame/NimGame.html#defining-the-training",
    "href": "projects/NimGame/NimGame.html#defining-the-training",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "3.3 Defining the training",
    "text": "3.3 Defining the training\nThe train function trains the AI by making it play against itself ‘n’ times. It sets up the game using Nim class, trains the AI using NimAI class, and finally returns the trained AI.\n\n\nCode\ndef train(n):\n    \"\"\"\n    Train an AI by playing `n` games against itself.\n    \"\"\"\n\n    player = NimAI()\n\n    # Play 'n' games\n    for i in range(n):\n        \n        # print the phrase only for the first and last 10 games when training is &gt; 50 games\n        if i+1 &lt;= 10 or n &lt;=50:\n            print(f\"Playing training game {i + 1}\")\n            \n        elif len(range(n)) - i &lt;= 10 :\n            if len(range(n)) - i == 10:\n                print(\"*\\n\"*5 , end='')\n            print(f\"Playing training game {i + 1}\")\n\n\n        #initialize the game\n        game = Nim()\n\n        # Keep track of last move made by either player\n        last = {\n            0: {\"state\": None, \"action\": None},\n            1: {\"state\": None, \"action\": None}\n        }\n\n        # Game loop\n        while True:\n\n            # Keep track of current state and action\n            state = game.piles.copy()\n            action = player.choose_action(game.piles)\n\n            # Keep track of last state and action\n            last[game.player][\"state\"] = state\n            last[game.player][\"action\"] = action\n\n            # Make move\n            game.move(action)\n            new_state = game.piles.copy()\n\n            # When game is over, update Q-value with rewards\n            if game.winner is not None:\n\n                # update the state and action that resulted in loss with reward value -1\n                player.update(state, action, new_state, -1)\n\n                # update the state and action that resulted in win with reward value 1\n                player.update(last[game.player][\"state\"], last[game.player][\"action\"], new_state, 1)\n                \n                break\n\n            # If game is continuing, update Q-value with no rewards i.e. 0\n            elif last[game.player][\"state\"] is not None:\n\n                player.update(last[game.player][\"state\"],last[game.player][\"action\"], new_state, 0)\n\n    print(\"\\nDone training\")\n\n    # Return the trained AI\n    return player"
  },
  {
    "objectID": "projects/NimGame/NimGame.html#defining-the-play-between-a-human-and-the-nimai",
    "href": "projects/NimGame/NimGame.html#defining-the-play-between-a-human-and-the-nimai",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "3.4 Defining the play between a human and the NimAI",
    "text": "3.4 Defining the play between a human and the NimAI\nThe play function sets up a Nim game between a human and the AI using the NimAI object.\n\n\nCode\ndef play(nim_ai, human_player=None):\n    \"\"\"\n    Play human game against the AI.\n    `human_player` can be set to 0 or 1 to specify whether\n    human player moves first or second.\n    \"\"\"\n\n    # If no player order set, choose human's order randomly\n    if human_player is None:\n        human_player = random.randint(0, 1)\n\n    # Create new game\n    game = Nim()\n\n    # Game loop\n    while True:\n\n        # Print contents of piles\n        print()\n        print(\"Piles:\")\n        for i, pile in enumerate(game.piles):\n            print(f\"Pile {i}: {pile}\")\n        print()\n\n        # Compute current available actions\n        available_actions = Nim.available_actions(game.piles)\n        time.sleep(1)\n\n        # Let human make a move\n        if game.player == human_player:\n            print(\"Your Turn\")\n            while True:\n                pile = int(input(\"Choose Pile: \"))\n                count = int(input(\"Choose Count: \"))\n                if (pile, count) in available_actions:\n                    break\n                print(\"Invalid move, try again.\")\n\n        # Have AI make a move \n        else:\n            print(\"AI's Turn\")\n            pile, count = nim_ai.choose_action(game.piles, epsilon=False)\n            print(f\"AI chose to take {count} object(s) from pile {pile}.\")\n\n        # updates the objects in each pile after taking action; Switches player; Checks for winner\n        game.move((pile, count))\n\n        # Checks for winner and ends the game\n        if game.winner is not None:\n            print()\n            print(\"GAME OVER\")\n            winner = \"Human\" if game.winner == human_player else \"AI\"\n            print(f\"Winner is {winner}\")\n            return"
  },
  {
    "objectID": "projects/NimGame/NimGame.html#human-vs-untrained-ai",
    "href": "projects/NimGame/NimGame.html#human-vs-untrained-ai",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "4.1 Human Vs Untrained AI",
    "text": "4.1 Human Vs Untrained AI\n\n\nCode\nuntrained_ai = train(0)\n\n\nDone training\n\n\nCode\n# start a Nim game between human and untrained AI\nplay(untrained_ai)\n\n\nPiles:\nPile 0: 1\nPile 1: 3\nPile 2: 5\nPile 3: 7\n\nYour Turn\nChoose Pile: 2\nChoose Count: 5\n\nPiles:\nPile 0: 1\nPile 1: 3\nPile 2: 0\nPile 3: 7\n\nAI's Turn\nAI chose to take 1 object(s) from pile 0.\n\nPiles:\nPile 0: 0\nPile 1: 3\nPile 2: 0\nPile 3: 7\n\nYour Turn\nChoose Pile: 3\nChoose Count: 6\n\nPiles:\nPile 0: 0\nPile 1: 3\nPile 2: 0\nPile 3: 1\n\nAI's Turn\nAI chose to take 1 object(s) from pile 3.\n\nPiles:\nPile 0: 0\nPile 1: 3\nPile 2: 0\nPile 3: 0\n\nYour Turn\nChoose Pile: 1\nChoose Count: 2\n\nPiles:\nPile 0: 0\nPile 1: 1\nPile 2: 0\nPile 3: 0\n\nAI's Turn\nAI chose to take 1 object(s) from pile 1.\n\nGAME OVER\nWinner is Human\nWe can see that it is very easy to win against the AI since it is playing randomly without using any optimized Q-values."
  },
  {
    "objectID": "projects/NimGame/NimGame.html#human-vs-trained-ai",
    "href": "projects/NimGame/NimGame.html#human-vs-trained-ai",
    "title": "Training an AI to play a game using reinforcement learning",
    "section": "4.2 Human Vs Trained AI",
    "text": "4.2 Human Vs Trained AI\nNow, let’s train our AI on 10,000 games and play against it.\n\n\nCode\ntrained_ai = train(10000)\n\n\nPlaying training game 1\nPlaying training game 2\nPlaying training game 3\nPlaying training game 4\nPlaying training game 5\nPlaying training game 6\nPlaying training game 7\nPlaying training game 8\nPlaying training game 9\nPlaying training game 10\n*\n*\n*\n*\n*\nPlaying training game 9991\nPlaying training game 9992\nPlaying training game 9993\nPlaying training game 9994\nPlaying training game 9995\nPlaying training game 9996\nPlaying training game 9997\nPlaying training game 9998\nPlaying training game 9999\nPlaying training game 10000\n\nDone training\n\n\nCode\n# start a Nim game between human and trained AI\nplay(trained_ai)\n\n\nPiles:\nPile 0: 1\nPile 1: 3\nPile 2: 5\nPile 3: 7\n\nAI's Turn\nAI chose to take 5 object(s) from pile 2.\n\nPiles:\nPile 0: 1\nPile 1: 3\nPile 2: 0\nPile 3: 7\n\nYour Turn\nChoose Pile: 1\nChoose Count: 2\n\nPiles:\nPile 0: 1\nPile 1: 1\nPile 2: 0\nPile 3: 7\n\nAI's Turn\nAI chose to take 6 object(s) from pile 3.\n\nPiles:\nPile 0: 1\nPile 1: 1\nPile 2: 0\nPile 3: 1\n\nYour Turn\nChoose Pile: 3\nChoose Count: 1\n\nPiles:\nPile 0: 1\nPile 1: 1\nPile 2: 0\nPile 3: 0\n\nAI's Turn\nAI chose to take 1 object(s) from pile 0.\n\nPiles:\nPile 0: 0\nPile 1: 1\nPile 2: 0\nPile 3: 0\n\nYour Turn\nChoose Pile: 1\nChoose Count: 1\n\nGAME OVER\nWinner is AI\nNow that we have trained the AI, we can see that it is quite challenging to beat the AI as it has gained the experience needed to make optimal moves and win the game."
  },
  {
    "objectID": "projects/Salesdashboard/salesdashboard.html",
    "href": "projects/Salesdashboard/salesdashboard.html",
    "title": "Sales dashboard using Tableau",
    "section": "",
    "text": "In this project, we will build a dashboard using Tableau to help stakeholders analyze sales performance for the years 2020-2023. The dashboard is designed based on the following requirements, and the complete dashboard can be accessed at the following link:\nClick here to view the full dashboard\n\n1 Dashboard Requirements\nSales metrics and trends:\n\nDisplay a summary of sales metrics such as total sales, total profits, and total quantity, comparing the current year with the previous year with KPI symbol.\nFor each sales metric, present the data on a monthly basis, comparing the current year with the previous year. Highlight the highest and lowest months.\n\nProduct subcategory comparison:\n\nCompare sales across different product subcategories for the current year and the previous year.\nCompare sales and profits for the current year.\n\nWeekly trends for sales & profit:\n\nPresent weekly sales and profit data for the current year.\nDisplay the average weekly values for sales and profit data.\nHighlight weeks that are above and below the average values to draw attention.\n\n\n\n2 Design & Interactivity Requirements\n\nMake the graphs interactive, enabling the users to filter the data using graphs.\nThe dashboard should allow users to check historical data by offering them the flexibility to select any desired year.\nAllow users to filter the data by product information like category and subcategory, as well as by location information such as region, state, and city.\n\n\n\n3 Sample GIF of the Dashboard\n\n\n\nGIF of the Dashboard\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html",
    "href": "projects/SQLMuseum/mfa.html",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "",
    "text": "In this project, we will explore the concepts of soft deletions, views, and triggers in the context of the Museum of Fine Arts (MFA). The MFA is a century-old museum housing numerous historical and contemporary artifacts and artworks. It manages its extensive collection through the MFA database, which tracks thousands of items. However, for the purpose of this project, we will work with a subset of the database containing only ten items in the collections table.\n\n\n\n\n\nER diagram of the MFA database\n\n\nThe Schema of the MFA database is shown above. The collections table contains the following columns:\n\nid, which is the ID of the table that serves as the primary key\ntitle, which is the name of the art piece\naccession_number, which is a unique ID used by the museum internally\nacquired, which indicates when the art was acquired\n\nLet us now establish the SQLite connection to the database using DBI package.\n\n\nCode\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"mfa.db\") # establish SQLite connection to the database\n\n\nThe data of the collections table is as follows:\n\n\nCode\nSELECT * FROM \"collections\";\n\n\n\nDisplaying records 1 - 10\n\n\nid\ntitle\naccession_number\nacquired\n\n\n\n\n1\nProfusion of flowers\n56.257\n1956-04-12\n\n\n2\nFarmers working at dawn\n11.6152\n1911-08-03\n\n\n3\nSpring outing\n14.76\n1914-01-08\n\n\n4\nImaginative landscape\n56.496\nNA\n\n\n5\nPeonies and butterfly\n06.1899\n1906-01-01\n\n\n6\nTile Lunette\n06.2437\n1906-11-08\n\n\n7\nStatuette of a shrew\n01.105\n1901-02-11\n\n\n8\nCountry road with culvert\n76.431\nNA\n\n\n9\nFamily of acrobats\n1974.352\n1933-03-30\n\n\n10\nBacchic scene with minotaur\n1974.379\n1933-05-18"
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#schema",
    "href": "projects/SQLMuseum/mfa.html#schema",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "",
    "text": "ER diagram of the MFA database\n\n\nThe Schema of the MFA database is shown above. The collections table contains the following columns:\n\nid, which is the ID of the table that serves as the primary key\ntitle, which is the name of the art piece\naccession_number, which is a unique ID used by the museum internally\nacquired, which indicates when the art was acquired\n\nLet us now establish the SQLite connection to the database using DBI package.\n\n\nCode\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"mfa.db\") # establish SQLite connection to the database\n\n\nThe data of the collections table is as follows:\n\n\nCode\nSELECT * FROM \"collections\";\n\n\n\nDisplaying records 1 - 10\n\n\nid\ntitle\naccession_number\nacquired\n\n\n\n\n1\nProfusion of flowers\n56.257\n1956-04-12\n\n\n2\nFarmers working at dawn\n11.6152\n1911-08-03\n\n\n3\nSpring outing\n14.76\n1914-01-08\n\n\n4\nImaginative landscape\n56.496\nNA\n\n\n5\nPeonies and butterfly\n06.1899\n1906-01-01\n\n\n6\nTile Lunette\n06.2437\n1906-11-08\n\n\n7\nStatuette of a shrew\n01.105\n1901-02-11\n\n\n8\nCountry road with culvert\n76.431\nNA\n\n\n9\nFamily of acrobats\n1974.352\n1933-03-30\n\n\n10\nBacchic scene with minotaur\n1974.379\n1933-05-18"
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#problem-1-soft-deleting-the-artworks",
    "href": "projects/SQLMuseum/mfa.html#problem-1-soft-deleting-the-artworks",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "2.1 Problem 1: Soft deleting the artworks",
    "text": "2.1 Problem 1: Soft deleting the artworks\n\n\n\n\n\n\nImplement a soft deletion of items in the collections table, where a log of sold artworks is kept in the column named “deleted” instead of completely removing them from the table, so that the records of artworks in the collection are not lost. The “deleted” column in the collections table must have a value of 0 for the available items for sale and a value of 1 for the items that have been sold. Imagine the artworks “Farmers Working at Dawn” and “Tile Lunette” were sold, and implement this idea of soft deletion on them."
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#problem-2-view-that-shows-only-available-artworks",
    "href": "projects/SQLMuseum/mfa.html#problem-2-view-that-shows-only-available-artworks",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "2.2 Problem 2: View that shows only available artworks",
    "text": "2.2 Problem 2: View that shows only available artworks\n\n\n\n\n\n\nCreate a view named current_collections using all the columns of the collections table except the “deleted” column, so that the view can be used to display only the information about the artworks that are available for sale."
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#problem-3-trigger-that-soft-deletes-the-artworks",
    "href": "projects/SQLMuseum/mfa.html#problem-3-trigger-that-soft-deletes-the-artworks",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "2.3 Problem 3: Trigger that soft deletes the artworks",
    "text": "2.3 Problem 3: Trigger that soft deletes the artworks\n\n\n\n\n\n\nSince data in the view cannot be modified directly, create a trigger on the current_collections view that soft deletes the data from the underlying collections table, as per the idea of soft deletion discussed above. The trigger must be activated when any data is attempted to be deleted from the view. Demonstrate this trigger by selling all the artworks that have no acquired date."
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#problem-4-trigger-that-reverses-the-soft-deletion-of-artworks",
    "href": "projects/SQLMuseum/mfa.html#problem-4-trigger-that-reverses-the-soft-deletion-of-artworks",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "2.4 Problem 4: Trigger that reverses the soft deletion of artworks",
    "text": "2.4 Problem 4: Trigger that reverses the soft deletion of artworks\n\n\n\n\n\n\nImagine the items sold in the task 3 are rebought. Now, create a trigger on the current_collections view that reverses the soft deletion, i.e., setting the corresponding row’s deleted value to 0 in the underlying collections table to indicate that the items are again available. The trigger must be executed when any soft-deleted data is attempted to be inserted into the current_collections view."
  },
  {
    "objectID": "projects/SQLMuseum/mfa.html#problem-5-trigger-that-inserts-new-artworks",
    "href": "projects/SQLMuseum/mfa.html#problem-5-trigger-that-inserts-new-artworks",
    "title": "Soft deletions, views, and triggers in the context of a museum’s database",
    "section": "2.5 Problem 5: Trigger that inserts new artworks",
    "text": "2.5 Problem 5: Trigger that inserts new artworks\n\n\n\n\n\n\nCreate a trigger on the current_collections view that inserts new data into the underlying collections table when any new data is attempted to be inserted into the view. Demonstrate this trigger by buying new artworks ‘Adoration of the Magi’ (accession_number: 1971.71, acquired: 2022-01-11) and ‘Agony in the Garden’ (accession_number: 68.206, acquired: 2022-05-01)."
  }
]